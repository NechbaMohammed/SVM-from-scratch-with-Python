{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76869e4",
   "metadata": {},
   "source": [
    "# <center> Machine À Vecteurs De Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1013ea6",
   "metadata": {},
   "source": [
    "## Qu’est-Ce Qu’une Machine À Vecteurs De Support ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf6274",
   "metadata": {},
   "source": [
    "<div>Une machine vectorielle de support ou SVM est un modèle d’apprentissage automatique supervisé. Les machines à vecteurs de support peuvent être utilisées à la fois pour la classification et pour les tâches de régression. Cependant, cet article ne couvrira que les machines vectorielles de support pour la classification. Un classificateur SVM fonctionne en calculant l’hyperplan qui sépare le mieux les classes dans les données d’entraînement. Une fois le modèle formé, la SVM utilisera la position de toutes les nouvelles instances par rapport à cet hyperplan séparateur afin de classer les instances. </div>\n",
    "<img src=\"figures/Graphique1.png\"> \n",
    "<div style=\"text-align: center;\"> Figure 1. Une illustration de l'hyperplan de séparation d'un SVM.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a8c76",
   "metadata": {},
   "source": [
    "## Quel est l’objectif du modèle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fde49",
   "metadata": {},
   "source": [
    "<div> Comme pour tout modèle de classification, l’objectif est de classer correctement les instances. C’est-à-dire prédire correctement la classe à laquelle appartient une instance donnée. Afin de classer les instances, le modèle doit avoir une limite de décision qui dicte les classes des instances qui se trouvent de chaque côté de celui-ci. La limite de décision choisie par une SVM est celle qui maximise les marges entre les classes.</div><br>\n",
    "<div>Afin d’effectuer la classification, le modèle calculera la position de l’instance donnée par rapport à l’hyperplan séparateur (qui est la limite de décision), puis affectera l’instance à la classe appropriée.</div><br>\n",
    "<div>La limite de décision d’une SVM sera toujours un hyperplan séparateur. Par conséquent, la limite de décision aura toujours une dimension inférieure à celle de l’espace d’entités dans lequel se trouvent les instances de formation.</div><br>\n",
    "<div>L’animation ci-dessous illustre la limite de décision de la SVM utilisée pour classer les instances. Comme l’espace d’entité dans lequel se trouvent les instances a trois dimensions, l’hyperplan séparateur est simplement un plan bidimensionnel.</div>\n",
    "<img src=\"figures/Linear3D.gif\">\n",
    "\n",
    "<div style=\"text-align: center;\"> Figure 2. L'hyperplan séparateur d'une SVM.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f291af",
   "metadata": {},
   "source": [
    "## Classement des marges dures (Hard Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15211a82",
   "metadata": {},
   "source": [
    "<div> L'idée derrière la classification des marges dures est de séparer parfaitement les classes dans les données d'entraînement. Lorsque la classification des marges dures est utilisée, le modèle tentera d'ajuster une limite de décision qui sépare parfaitement les deux classes dans les données d'apprentissage. Par conséquent, il est clair que la classification des marges dures n'est pas possible si les classes ne sont pas linéairement séparables.</div><br>\n",
    "<div>La classification des marges dures entraînera souvent une variance élevée, car le SVM modélisera le bruit dans l'ensemble de données. La classification des marges dures est sensible aux valeurs aberrantes et aura tendance to overfit the training data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d7375",
   "metadata": {},
   "source": [
    "## Classification des marges souples (soft Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f937af3",
   "metadata": {},
   "source": [
    "<div>La classification des marges souples équilibre le compromis entre avoir la plus grande marge possible et avoir le moins de violations de marge possible. Une violation de marge se produit lorsqu'une instance se trouve dans la marge ou du mauvais côté de la limite de décision. </div><br>\n",
    "<div> Afin de calculer la frontière de décision optimale d'un SVM, nous devons résoudre un problème d'optimisation quadratique convexe qui a des contraintes linéaires. Le problème d'optimisation SVM linéaire à marge dure doit être étendu pour permettre les violations de marge.</div><br>\n",
    "<div>Pour ce faire, nous ajoutons des variables d'écart au problème d'optimisation. Les variables d'écart permettent d'inclure ces points dans le problème et de les pondérer à l'aide d'un hyperparamètre C. L'effet du paramètre C sur la marge est le suivant, avec une augmentation de la valeur de C, le poids des points mal classés est augmenté provoquant la réduction de la marge. Une valeur élevée de C augmentera les chances que le modèle overfit les données d'apprentissage, donc si tel est le cas, vous pouvez régulariser le modèle en diminuant la valeur de C. </div><br>\n",
    "<div>Remarquez l'effet de la valeur de C sur les limites de décision choisies par les trois machines à vecteurs de support ci-dessous. Les points marqués de croix sont les vecteurs supports des modèles. </div>\n",
    "<img src=\"figures/C10.png\">\n",
    "<div style=\"text-align:center\"> Figure 3. Une illustration de l'effet de l'hyperparamètre C sur un SVM.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094ea0e",
   "metadata": {},
   "source": [
    "## Les Mathématiques Derrière L'objectif De Classificateur De Machine À Vecteur De Support Linéaire À Marge Dure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf480e",
   "metadata": {},
   "source": [
    "<div>Je vais cependant donner un aperçu de base du sujet. Je développerai également certains points qui peuvent s'avérer difficiles à suivre dans le cours magistral, comme la formule de la largeur de la marge.</div><br>\n",
    "<strong>Maximiser La Largeur De La Marge.</strong>\n",
    "<img src=\"figures/figure4.png\">\n",
    "<div  style=\"text-align:center\">Figure 4.</div><br>\n",
    "<div>Considérez la figure 4, supposons que toutes les croix vertes appartiennent à la classe négative et que tous les cercles rouges appartiennent à la classe positive.</div>\n",
    "<img src=\"figures/figure5.png\">\n",
    "<div style=\"text-align:center\">Figure 5.</div><br>\n",
    "<div>La figure 5 illustre la limite de décision optimale (ligne violet clair) qui maximise la largeur de la marge.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ffdd",
   "metadata": {},
   "source": [
    "<img src=\"figures/figure6.png\">\n",
    "<div style=\"text-align:center\">Figure 6</div><br>\n",
    "<div>La figure 6, représente géométriquement un des vecteurs de chaque classe. $x_+$ désigne une instance de la classe positive située au bord de la marge. De la même manière, $x_-$désigne une instance de la classe négative située au bord de la marge. Le vecteur qui couvre la distance entre $x_+$ et $x_-$ est le vecteur $x_+-x_-$. Notez que ce vecteur n'est pas perpendiculaire à la marge et donc la magnitude de ce vecteur n'est pas égale à la largeur de la marge.</div>\n",
    "<img src=\"figures/figure7.png\">\n",
    "<div align=\"center\"> Figure 7</div><br>\n",
    "<div> Pour surmonter cela, nous ajouterons le vecteurw, qui est un vecteur normal à la marge. La largeur de la marge est alors égale au produit scalaire,$x_+-x_{-} . \\frac{w}{||w||}$. Afin de comprendre pourquoi il en est ainsi, nous allons considérer l'interprétation géométrique du produit scalaire.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e3101",
   "metadata": {},
   "source": [
    "<div> Le produit scalaire des deux vecteurs $u$ et $v$ peut être interprété comme une mise à l'échelle du vecteur unitaire dans la direction de $v$ par la projection de $u$ sur $v$.</div>\n",
    "<img src=\"figures/figure8.png\">\n",
    "<div align=\"center\"> Fugure 8</div><br>\n",
    "<div> Le vecteur $w$ est mis à l'échelle par la ligne orange dans la direction dewdans la figure 8. Par conséquent, la largeur de la marge est égale à l'unité normale à la marge mise à l'échelle par la projection de $(x_+–x_−)$ sur $w$.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49901d",
   "metadata": {},
   "source": [
    "Ainsi, $ \\text {width} = (x_+-x_{-}) . \\frac{w}{||w||}$<br>\n",
    "Cela peut être simplifié en, $\\text { width }=\\frac{2}{\\|w\\|} \\text {. }$\n",
    "<div> Nous voulons maximiser la largeur de la marge, donc nous voulons minimiser $||w||$, mais cela revient à minimiser $\\frac{1}{2}||w||^\\frac{1}{2}$ </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165b426",
   "metadata": {},
   "source": [
    "<div> Par conséquent, l'objectif de la machine à vecteur de support linéaire à marge dure est le suivant, $\\min _{w, b} \\frac{1}{2}\\|w\\|^{\\frac{1}{2}}$ s.t. $y^{(i)}\\left(x^{(1)} \\cdot w+b\\right) \\geq 1$ pour $i=1,2, \\cdots, m$\n",
    "<div> Nous pouvons ensuite utiliser l'objectif ci-dessus pour atteindre l'objectif de la machine à vecteur de support linéaire à marge souple qui est le suivant,\n",
    " $$\n",
    "\\min _{w, b, \\xi} \\frac{1}{2}\\|w\\|^{\\frac{1}{2}}+C \\sum_{i=1}^m \\xi^{(i)}\n",
    "$$\n",
    "s.t. $\\quad y^{(i)}\\left(x^{(i)} \\cdot w+b\\right) \\geq 1-\\xi^{(i)}$ for $i=1,2, \\cdots, m$\n",
    "et $\\xi^{(1)} \\geq 0$ for $i=1,2, \\ldots, m$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4be2b",
   "metadata": {},
   "source": [
    "<strong>Le Problème Dual</strong><br>\n",
    "<div> Nous allons résoudre la forme duale de l'objectif SVM linéaire. Cela donnera les multiplicateurs lagrangiens, qui seront utilisés pour calculer la valeur optimale $w$ et $b$.</div><br>\n",
    "<div> La forme duale de l'objectif SVM linéaire :\n",
    "    $$\n",
    "\\min _\\alpha \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha^{(i)} \\alpha^{(j)} y^{(i)} y^{(j)} x^{(i)} \\cdot x^{(j)}-\\sum_{i=1}^m \\alpha^{(i)}\n",
    "$$\n",
    "    <div align=\"center\">s.t. $\\alpha^{(i)} \\geq 0$ pour $i=1,2, \\cdots, m$</div>\n",
    "Optimale $w$ et $b$,\n",
    "$$\n",
    "\\begin{gathered}\n",
    "w^*=\\sum_{i=1}^m \\alpha^{*(i)} y^{(i)} x^{(i)} \\\\\n",
    "b^*=\\frac{1}{n_s} \\sum_{\\substack{i=1 \\\\\n",
    "a^*(i) > 0}}^m\\left(y^{(i)}-w^* \\cdot x^{(i)}\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "<strong> Faire Des Prédictions.</strong><br>\n",
    "<div> La prédiction d'un SVM est calculée comme suit,\n",
    "$$\\hat{y}=\\left\\{\\begin{array}{lll}\n",
    "0 & \\text { si } & w \\cdot x+b<0 \\\\\n",
    "1 & \\text { si } & w \\cdot x+b \\geq 1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "où $\\hat{y}$ est la classe prédite de l'instance $x$. \n",
    "    </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3dce4d",
   "metadata": {},
   "source": [
    " <strong> Données Non Linéaires.</strong><br>\n",
    " \n",
    "<div>La méthode utilisée par les machines à vecteurs de support pour classer les données non linéaires consiste à mapper les données dans un espace de dimension supérieure. Les fonctions sont utilisées pour mapper les données dans un espace dimensionnel supérieur avant la classification dans l'espoir que les données seront linéairement séparables dans l'espace des caractéristiques transformées. L'hyperplan de séparation sera calculé dans l'espace des caractéristiques de dimension supérieure. Cela permet une frontière de décision non linéaire dans l'espace des caractéristiques d'origine des données d'apprentissage.</div>\n",
    " <img src=\"figures/figure9.png\">\n",
    "    <div align=\"center\">Figure 9</div><br>\n",
    "    <div> Figure 9. Illustration de la façon dont les données peuvent être linéairement séparables après avoir été mappées dans un espace de dimension supérieure</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3717d8",
   "metadata": {},
   "source": [
    "<strong> Noyau.</strong><br>\n",
    "    \n",
    "<div>Soit $a$ et $b$ des instances dans l'espace de caractéristiques d'origine et soit $\\phi$ une fonction de mappage.</div><br>\n",
    "<div> Un noyau est une fonction capable de calculer le produit scalaire des vecteurs transformés, $\\phi(a)$ et $\\phi(b)$ sans avoir à transformer les vecteurs dans l'espace des caractéristiques de dimension supérieure. Ainsi, une fonction noyau est capable de calculer le produit scalaire $\\phi(a).\\phi(b)$\n",
    "n'utilisant que les vecteurs $\\phi(a)$ et $\\phi(b)$. Les noyaux utilisent l'astuce du noyau pour permettre un calcul beaucoup plus efficace.</div><br>\n",
    "    <strong> Noyau Polynomial.</strong>\n",
    "   <div>Le noyau polynomial peut être utilisé pour ajouter des caractéristiques polynomiales à l'ensemble de données. J'ai écrit un article complet expliquant ce que sont les caractéristiques polynomiales.</div><br>\n",
    "$$\n",
    "K(a,b)=(a.b+r)^d\n",
    "$$\n",
    "    <strong>Noyau RBF Gaussien. </strong><br>\n",
    "    \n",
    "<div> Une méthode utilisée pour gérer les données non linéaires consiste à ajouter des fonctionnalités de similarité. Une fonction est utilisée pour quantifier la similarité entre les points de l'ensemble de données, la valeur de similarité calculée est ensuite ajoutée à l'ensemble de données en tant que caractéristique.</div><br>\n",
    "<div> Il est courant de mesurer la similarité entre chaque point de l'ensemble de données et tous les autres points de l'ensemble de données. Ceci est fait car cela augmente les chances que l'ensemble de données devienne linéairement séparable. Cependant, cela se fait au prix d'une forte augmentation du nombre de fonctionnalités dans l'ensemble de données, en supposant que l'ensemble d'apprentissage comporte un grand nombre d'instances.</div><br>\n",
    " \n",
    "<div> Une fonction couramment utilisée pour calculer la similarité entre les instances est la fonction de base radiale gaussienne (RBF gaussienne). Le noyau RBF permet à l'utilisateur de modifier l'hyperparamètre gamma. Plus la valeur de gamma est élevée, plus la courbe utilisée est pointue et plus la frontière de décision est irrégulière. Par conséquent, si votre modèle est underfitting aux données d'apprentissage, augmentez la valeur de gamma. Si votre modèle overfitting les données, réduisez la valeur de gamma.</div><br>\n",
    "$$\n",
    "K(a,b)=\\exp(-\\gamma ||a-b||^2)\n",
    "$$\n",
    "<strong> Machines À Vecteurs De Support Noyaux.</strong><br>\n",
    "  $$\n",
    " \\begin{aligned}\n",
    "&b^*=\\frac{1}{n_s} \\sum_{\\substack{i=1 \\\\\n",
    "\\alpha^*(i)>0}}^m\\left(y^{(i)}-\\sum_{\\substack{j=1 \\\\\n",
    "\\alpha^*(j)>0}}^m \\alpha^{*(j)} y^{(j)} K\\left(x^{(i)}, x^{(j)}\\right)\\right) \\\\\n",
    "&\\hat{y}=\\left\\{\\begin{array}{lll}\n",
    "0 & \\text { if } \\sum_{\\substack{i=1 \\\\\n",
    "\\alpha^*(i)>0}}^m\\alpha^{*(i)} y^{(i)} K\\left(x^{(i)}, x\\right)+b^*<0 \\\\\n",
    "1 & \\text { if } \\sum_{\\substack{i=1 \\\\\n",
    "\\alpha^*(i)>0}}^m \\alpha^{*(i)} y^{(i)} K\\left(x^{(i)}, x\\right)+b^* \\geq 0\n",
    "\\end{array}\\right.\n",
    "\\end{aligned}\n",
    " $$\n",
    "<img src=\"figures/figure10.png\">\n",
    "<div align=\"center\">Figure 10</div>\n",
    "<div> Figure 10. Classification non linéaire effectuée par un SVM. Le tracé de contour illustre la confiance du modèle.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
